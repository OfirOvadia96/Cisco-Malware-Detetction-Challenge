{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np # Support for large arrays and matrices, along with high-level mathematical functions.\n",
    "import seaborn as sns # Graphing/Plotting module.\n",
    "import pandas as pd # CSV handling with operations on tabular data.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from ast import literal_eval # Transform/Parse a string-list into a proper list.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "plt.rc(\"font\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.shape:  (20493, 91)\n",
      "df_test.shape:  (5856, 88)\n",
      "df_val.shape:  (2927, 88)\n"
     ]
    }
   ],
   "source": [
    "# Read Data\n",
    "''' \n",
    "There are 2 datasets available for you to use - \n",
    "1. 'mta' \n",
    "2. 'ustc'\n",
    "Make sure that \n",
    "the path to the .csv files is correct. the following code loads the data from .csv file\n",
    "into a DataFrame.\n",
    "'''\n",
    "\n",
    "dataset_type = 'mta' # 'mta' or 'ustc'\n",
    "\n",
    "filepath = f'./datasets/{dataset_type}/xy_train.csv'\n",
    "df = pd.read_csv(filepath)\n",
    "print(\"df_train.shape: \",df.shape)\n",
    "\n",
    "filepath_test = f'./datasets/{dataset_type}/x_test.csv'\n",
    "df_test = pd.read_csv(filepath_test)\n",
    "print(\"df_test.shape: \",df_test.shape)\n",
    "\n",
    "filepath_val = f'./datasets/{dataset_type}/x_val.csv'\n",
    "df_val = pd.read_csv(filepath_val)\n",
    "print(\"df_val.shape: \",df_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"malware or benign: \\n\", df['label'].value_counts())\n",
    "print(\"\\nmalware family:  \\n\", df['malware_family'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(30,8)})\n",
    "sns.countplot(x='malware_family', data=df, palette='hls')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['application_category_name'].nunique()\n",
    "\n",
    "pd.crosstab(df.application_category_name,df.label).plot(kind='bar')\n",
    "plt.title('application_category_name')\n",
    "plt.xlabel('application_category_name')\n",
    "plt.ylabel('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear unnecessary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert matrix to array and sum\n",
    "\n",
    "import numpy as np  \n",
    "\n",
    "#Convert string to array\n",
    "df['udps.n_bytes'] = df['udps.n_bytes'].transform(literal_eval)\n",
    "df['udps.protocol_header_fields'] = df['udps.protocol_header_fields'].transform(literal_eval)\n",
    "df['udps.n_bytes_per_packet'] = df['udps.n_bytes_per_packet'].transform(literal_eval)\n",
    "df['udps.stnn_image'] = df['udps.stnn_image'].transform(literal_eval)\n",
    "# ## Flat the matrix to array\n",
    "df['udps.protocol_header_fields'] = df['udps.protocol_header_fields'].apply(lambda a: np.array(a).flatten())\n",
    "df['udps.n_bytes_per_packet'] = df['udps.n_bytes_per_packet'].apply(lambda a: np.array(a).flatten())\n",
    "df['udps.stnn_image'] = df['udps.stnn_image'].apply(lambda a: np.array(a).flatten())\n",
    "\n",
    "\n",
    "# # #Sum array\n",
    "# for row in df['udps.n_bytes']:\n",
    "#     df[row] = np.sum(row)\n",
    "    # pd.concat((np.sum(row),row),axis=0,ignore_index=True)\n",
    "\n",
    "# for row in df['udps.protocol_header_fields']:\n",
    "#     # df[row] = np.sum(row,axis=1)\n",
    "#     pd.concat((np.sum(row,axis=1),row),axis=0, ignore_index=True)\n",
    "\n",
    "# for row in df['udps.n_bytes_per_packet']:\n",
    "#     pd.concat((np.sum(row,axis=1),row),axis=0, ignore_index=True)\n",
    "\n",
    "# for row in df['udps.stnn_image']:\n",
    "#     pd.concat((np.sum(row,axis=1),row),axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - After:  (20493, 80)\n",
      "Test - After:  (5856, 77)\n",
      "Val - After:  (2927, 77)\n"
     ]
    }
   ],
   "source": [
    "#Drop the columns where the values are equal in all\n",
    "for col in df.columns:\n",
    "    if len(df[col].unique()) == 1:\n",
    "        # print(\"col drop: \",col)\n",
    "        df = df.drop(col,axis=1)\n",
    "\n",
    "print(\"Train - After: \" ,df.shape)\n",
    "\n",
    "#------------------------------------\n",
    "\n",
    "#Drop the columns where the values are equal in all\n",
    "for col in df_test.columns:\n",
    "    if len(df_test[col].unique()) == 1:\n",
    "        # print(\"col drop: \",col)\n",
    "        df_test = df_test.drop(col,axis=1)\n",
    "\n",
    "print(\"Test - After: \" ,df_test.shape)\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "#Drop the columns where the values are equal in all\n",
    "for col in df_val.columns:\n",
    "    if len(df_val[col].unique()) == 1:\n",
    "        # print(\"col drop: \",col)\n",
    "        df_val = df_val.drop(col,axis=1)\n",
    "\n",
    "print(\"Val - After: \" ,df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert value of malware to 1 and benign to 0 \n",
    "df['label']=np.where(df['label'] =='malware', '1', df['label'])\n",
    "df['label']=np.where(df['label'] =='benign', '0', df['label'])\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop string from xy_train\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
    "\n",
    "#-----------Train-------------\n",
    "y = df['label']\n",
    "df.drop('src_ip', inplace=True, axis=1)\n",
    "df.drop('dst_ip', inplace=True, axis=1)\n",
    "df.drop('src_mac', inplace=True, axis=1)\n",
    "df.drop('src_oui', inplace=True, axis=1)\n",
    "df.drop('dst_mac', inplace=True, axis=1)\n",
    "df.drop('dst_oui', inplace=True, axis=1)\n",
    "df.drop('application_name', inplace=True, axis=1)\n",
    "df.drop('application_category_name', inplace=True, axis=1)\n",
    "df.drop('requested_server_name', inplace=True, axis=1)\n",
    "df.drop('client_fingerprint', inplace=True, axis=1)\n",
    "df.drop('server_fingerprint', inplace=True, axis=1)\n",
    "df.drop('content_type', inplace=True, axis=1)\n",
    "df.drop('user_agent', inplace=True, axis=1)\n",
    "df.drop('udps.protocol_header_fields', inplace=True, axis=1)\n",
    "df.drop('udps.n_bytes_per_packet', inplace=True, axis=1)\n",
    "df.drop('udps.stnn_image', inplace=True, axis=1)\n",
    "df.drop('udps.n_bytes', inplace=True, axis=1)\n",
    "df.drop('file_name', inplace=True, axis=1)\n",
    "df.drop('label', inplace=True, axis=1)\n",
    "df.drop('malware_family', inplace=True, axis=1)\n",
    "\n",
    "#-----------Test-------------\n",
    "\n",
    "df_test.drop('src_ip', inplace=True, axis=1)\n",
    "df_test.drop('dst_ip', inplace=True, axis=1)\n",
    "df_test.drop('src_mac', inplace=True, axis=1)\n",
    "df_test.drop('src_oui', inplace=True, axis=1)\n",
    "df_test.drop('dst_mac', inplace=True, axis=1)\n",
    "df_test.drop('dst_oui', inplace=True, axis=1)\n",
    "df_test.drop('application_name', inplace=True, axis=1)\n",
    "df_test.drop('application_category_name', inplace=True, axis=1)\n",
    "df_test.drop('requested_server_name', inplace=True, axis=1)\n",
    "df_test.drop('client_fingerprint', inplace=True, axis=1)\n",
    "df_test.drop('server_fingerprint', inplace=True, axis=1)\n",
    "df_test.drop('content_type', inplace=True, axis=1)\n",
    "df_test.drop('user_agent', inplace=True, axis=1)\n",
    "df_test.drop('udps.protocol_header_fields', inplace=True, axis=1)\n",
    "df_test.drop('udps.n_bytes_per_packet', inplace=True, axis=1)\n",
    "df_test.drop('udps.stnn_image', inplace=True, axis=1)\n",
    "df_test.drop('udps.n_bytes', inplace=True, axis=1)\n",
    "\n",
    "#-----------Validation-------------\n",
    "\n",
    "df_val.drop('src_ip', inplace=True, axis=1)\n",
    "df_val.drop('dst_ip', inplace=True, axis=1)\n",
    "df_val.drop('src_mac', inplace=True, axis=1)\n",
    "df_val.drop('src_oui', inplace=True, axis=1)\n",
    "df_val.drop('dst_mac', inplace=True, axis=1)\n",
    "df_val.drop('dst_oui', inplace=True, axis=1)\n",
    "df_val.drop('application_name', inplace=True, axis=1)\n",
    "df_val.drop('application_category_name', inplace=True, axis=1)\n",
    "df_val.drop('requested_server_name', inplace=True, axis=1)\n",
    "df_val.drop('client_fingerprint', inplace=True, axis=1)\n",
    "df_val.drop('server_fingerprint', inplace=True, axis=1)\n",
    "df_val.drop('content_type', inplace=True, axis=1)\n",
    "df_val.drop('user_agent', inplace=True, axis=1)\n",
    "df_val.drop('udps.protocol_header_fields', inplace=True, axis=1)\n",
    "df_val.drop('udps.n_bytes_per_packet', inplace=True, axis=1)\n",
    "df_val.drop('udps.stnn_image', inplace=True, axis=1)\n",
    "df_val.drop('udps.n_bytes', inplace=True, axis=1)\n",
    "\n",
    "# # FOR ustc\n",
    "# df.drop('Unnamed: 0.1', inplace=True, axis=1)\n",
    "# df.fillna(0)\n",
    "# df_test.drop('Unnamed: 0.1', inplace=True, axis=1)\n",
    "# df_test.fillna(0)\n",
    "# df_val.drop('Unnamed: 0.1', inplace=True, axis=1)\n",
    "# df_val.fillna(0)\n",
    "\n",
    "X = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "import sklearn.ensemble as ek\n",
    "\n",
    "extratrees = ek.ExtraTreesClassifier().fit(X,y)\n",
    "model = SelectFromModel(extratrees, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "nbfeatures = X_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "features = []\n",
    "index = numpy.argsort(extratrees.feature_importances_)[::-1][:nbfeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(nbfeatures):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[2+index[f]], extratrees.feature_importances_[index[f]]))\n",
    "    features.append(X.columns[2+f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_new\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features with low variance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# NEED ONLY INT\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(df)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate feature selection\n",
    "\n",
    "#Select features according to the k highest scores.\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "X = SelectKBest(chi2, k=10).fit_transform(df, y)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = GenericUnivariateSelect(chi2, mode='k_best', param=10)\n",
    "X = transformer.fit_transform(df, y)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "There are 2 type of labels:\n",
    "1. 'label': Binary labelling of {benign, malware} \n",
    "2. 'malware_family': Multi-class labelling of benign and malware families.\n",
    "'''\n",
    "\n",
    "label_type = 'label' # or 'malware_family' for malware classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the model with 42 trees\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "model_type = \"Decision Tree Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt')\n",
    "\n",
    "model_type = \"Random Forest Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoostClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create the model with 100 trees\n",
    "model = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "model_type = \"AdaBoost Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#n_estimators - 100 decision stumps as weak learners\n",
    "#max_leaf_nodes - number of leaf in the tree\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "\n",
    "model_type = \"Gradient Boosting Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model Fitting - NOT GOOD***\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model_type = \"Logistic Regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------xy_train------------------\n",
    "# Fit on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Actual class predictions\n",
    "y_predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------y_test------------------\n",
    "\n",
    "# # Fit on all train data\n",
    "model.fit(X,y)\n",
    "\n",
    "# Actual class predictions\n",
    "y_predictions = model.predict(df_test)\n",
    "\n",
    "enc = LabelEncoder()\n",
    "np.savetxt('predictions_test_binary_'+ dataset_type +'.txt', enc.fit_transform(y_predictions), fmt='%2d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------y_val------------------\n",
    "\n",
    "# Fit on all train data\n",
    "model.fit(X,y)\n",
    "\n",
    "# Actual class predictions\n",
    "y_predictions = model.predict(df_val)\n",
    "\n",
    "enc = LabelEncoder()\n",
    "np.savetxt('predictions_val_binary_'+ dataset_type +'.txt', enc.fit_transform(y_predictions), fmt='%2d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of ' + model_type + ' on test set: {:.5f}'.format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Confusion Matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_predictions)\n",
    "# print(confusion_matrix)\n",
    "\n",
    "true_labels = y_test\n",
    "heatmap = sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g', \n",
    "                      xticklabels=np.unique(true_labels), \n",
    "                      yticklabels=np.unique(true_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# K FOLD - CROSS VALIDATION\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "    random_state=0)\n",
    "scores = cross_val_score(clf, df, y, cv=5)\n",
    "print(\"DecisionTreeClassifier - scores.mean(): \",scores.mean())\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, df, y, cv=5)\n",
    "print(\"RandomForestClassifier - scores.mean(): \",scores.mean())\n",
    "\n",
    "\n",
    "# n_estimators - Number of Tree (More its better)\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, df, y, cv=5)\n",
    "print(\"ExtraTreesClassifier - scores.mean(): \",scores.mean())\n",
    "\n",
    "\n",
    "# AdaBoostClassifier\n",
    "# n_estimators = 100 weak learners\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, df, y, cv=5)\n",
    "print(\"AdaBoostClassifier - scores.mean(): \",scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf1 = DecisionTreeClassifier(random_state=42)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "clf3 = AdaBoostClassifier(n_estimators=100)\n",
    "clf4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('rf', clf2), ('ab', clf3), ('gb', clf4)], voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, eclf], ['Decision Tree', 'Random Forest', 'AdaBoost', 'GradientBoosting']):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.10f (+/- %0.10f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "116f2652fbe7bd71c42006778ec45c04a73572d75cb877527c86e96111d40ce9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
